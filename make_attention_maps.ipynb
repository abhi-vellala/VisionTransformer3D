{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a3a5a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from monai.transforms import Compose, ScaleIntensity, EnsureChannelFirst, ToTensor, Rotate90\n",
    "from dataloader import CTScanData\n",
    "from vit_model import VisionTransformer3D, UpsampleAttentionMap\n",
    "import random\n",
    "import nibabel as nib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7772cde2-b4f6-447b-9fb2-35474d5df826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e45b9a5-ec78-4b3e-9a85-0231d95525ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove 'module.' prefix from state_dict keys\n",
    "def remove_module_prefix(state_dict):\n",
    "    new_state_dict = {}\n",
    "    for k, v in state_dict.items():\n",
    "        if k.startswith('module.'):\n",
    "            new_state_dict[k[7:]] = v\n",
    "        else:\n",
    "            new_state_dict[k] = v\n",
    "    return new_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4810eac8-9728-42e1-b7a2-ab33facc2a82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer3D(\n",
       "  (patch_embedding): PatchEmbedding3D(\n",
       "    (proj): Conv3d(1, 256, kernel_size=(16, 16, 16), stride=(16, 16, 16))\n",
       "  )\n",
       "  (transformerlayers): ModuleList(\n",
       "    (0-7): 8 x TransformerBlock(\n",
       "      (attention_layer): Attention(\n",
       "        (qkv_layer): Linear(in_features=256, out_features=768, bias=False)\n",
       "        (dropout_layer): Dropout(p=0.0, inplace=False)\n",
       "        (softmax_layer): Softmax(dim=-1)\n",
       "        (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (linear_layer): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (feedforward_layer): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (2): GELU(approximate='none')\n",
       "          (3): Dropout(p=0.0, inplace=False)\n",
       "          (4): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (5): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  (last_linear_layer): Linear(in_features=256, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model\n",
    "in_channels = 1\n",
    "d_model = 256\n",
    "feedforward_dim = 512\n",
    "num_heads = 8\n",
    "patch_size = 16\n",
    "num_layers = 8\n",
    "num_classes = 2\n",
    "\n",
    "model = VisionTransformer3D(in_channels, d_model, feedforward_dim, patch_size, num_classes=num_classes, num_layers=num_layers)\n",
    "state_dict = torch.load('./Data/results_2/epoch_430_model.pt')\n",
    "state_dict = remove_module_prefix(state_dict)  # Remove 'module.' prefix\n",
    "model.load_state_dict(state_dict)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13bc3055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 224, 224, 224)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(224, 224, 224)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load attention map\n",
    "attn_map_path = './Data/results_2/279_attn_map.pt'\n",
    "attention_map = torch.load(attn_map_path).cpu().numpy()\n",
    "print(attention_map.shape)\n",
    "# Convert attention map to a 2D map (sum over the depth axis)\n",
    "attention_map_2d = attention_map.sum(axis=0)\n",
    "\n",
    "# Normalize the attention map for better visualization\n",
    "attention_map_2d_normalized = (attention_map_2d - np.min(attention_map_2d)) / (np.max(attention_map_2d) - np.min(attention_map_2d))\n",
    "attention_map_2d_normalized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4905c518-dbb6-47f4-9235-de312b4cc334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_excel('./Data/image_data.xlsx')\n",
    "train_transforms = Compose([\n",
    "    ScaleIntensity(),\n",
    "#     Rotate90(),\n",
    "    EnsureChannelFirst(channel_dim=\"no_channel\"),\n",
    "    ToTensor()\n",
    "])\n",
    "train_dataset = CTScanData(df, transform=train_transforms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee50cfd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['idx', 'ID', 'image', 'seg', 'Age', 'target'], dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rename(columns={'Unnamed: 0': 'idx'}, inplace=True)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b8c0085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TARGET: 1\n",
      "[1, 3, 4, 6, 9, 11, 12, 14, 16, 20, 21, 22, 23, 30, 32, 34, 45, 46, 47, 49, 52, 54, 61, 63, 64, 67, 71, 75, 76, 78, 79, 80, 83, 85, 87, 89]\n",
      "*******************************************************************************************************************************\n",
      "TARGET: 0\n",
      "[0, 2, 5, 7, 8, 10, 13, 15, 17, 18, 19, 24, 25, 26, 27, 28, 29, 31, 33, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 48, 50, 51, 53, 55, 56, 57, 58, 59, 60, 62, 65, 66, 68, 69, 70, 72, 73, 74, 77, 81, 82, 84, 86, 88]\n"
     ]
    }
   ],
   "source": [
    "print('TARGET: 1')\n",
    "print(list(df['idx'][df['target']==1]))\n",
    "print('*'*127)\n",
    "print('TARGET: 0')\n",
    "print(list(df['idx'][df['target']==0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e05a9589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx: 52, ID: 53, Age: 75\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 224, 224, 224])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_class1 = random.choice(list(df['idx'][df['target']==1]))\n",
    "print(f\"idx: {idx_class1}, ID: {df['ID'][idx_class1]}, Age: {df['Age'][idx_class1]}\")\n",
    "class1_age = df['Age'][idx_class1]\n",
    "class1_id = df['ID'][idx_class1]\n",
    "image_class1, _ = train_dataset[idx_class1]\n",
    "image_class1 = image_class1.unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "image_class1.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7f80c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx: 27, ID: 28, Age: 58\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 224, 224, 224])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_class0 = random.choice(list(df['idx'][df['target']==0]))\n",
    "print(f\"idx: {idx_class0}, ID: {df['ID'][idx_class0]}, Age: {df['Age'][idx_class0]}\")\n",
    "class0_age = df['Age'][idx_class0]\n",
    "class0_id = df['ID'][idx_class0]\n",
    "image_class0, _ = train_dataset[idx_class0]\n",
    "image_class0 = image_class0.unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "image_class0.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "130c0ec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224, 224, 224)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_attn_map = attention_map_2d_normalized.copy()\n",
    "final_attn_map.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3041a037",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_values(value):\n",
    "    if value < 0.05:\n",
    "        return 0\n",
    "    elif value < 0.1:\n",
    "        return 1\n",
    "    elif value < 0.2:\n",
    "        return 2\n",
    "    elif value < 0.3:\n",
    "        return 3\n",
    "    elif value < 0.4:\n",
    "        return 4\n",
    "    elif value < 0.5:\n",
    "        return 5\n",
    "    elif value < 0.6:\n",
    "        return 6\n",
    "    elif value < 0.7:\n",
    "        return 7\n",
    "    elif value < 0.8:\n",
    "        return 8\n",
    "    elif value < 0.9:\n",
    "        return 9\n",
    "    elif value <= 1.0:\n",
    "        return 10\n",
    "    else:\n",
    "        return -1  # For values outside the expected range\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6428be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_map_values = np.vectorize(map_values)\n",
    "famap = vectorized_map_values(final_attn_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35ee1e97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(famap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8f797e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save files\n",
    "img_load = nib.load(\"./Data/Resampled/1_resampled_img.nii.gz\")\n",
    "# class0\n",
    "image_class0_nii = nib.Nifti1Image(image_class0.cpu().numpy().squeeze(), img_load.affine)\n",
    "nib.save(image_class0_nii, f'./Data/view_attn_maps/image_class0_id{class0_id}_age{class0_age}.nii.gz')\n",
    "\n",
    "#class1\n",
    "image_class1_nii = nib.Nifti1Image(image_class1.cpu().numpy().squeeze(), img_load.affine)\n",
    "nib.save(image_class1_nii, f'./Data/view_attn_maps/image_class1_id{class1_id}_age{class1_age}.nii.gz')\n",
    "\n",
    "#attn map\n",
    "# attn_map_nii = nib.Nifti1Image(famap, img_load.affine)\n",
    "# nib.save(attn_map_nii, './Data/view_attn_maps/final_attention_map.nii.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea96f96e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vit",
   "language": "python",
   "name": "vit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
